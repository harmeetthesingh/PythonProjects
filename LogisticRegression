!pip install scipy==0.18.1
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
%matplotlib inline
from google.colab import drive
drive.mount('/content/drive')


def load_dataset():

  train_dataset = h5py.File('/content/drive/My Drive/Colab Notebooks/train_catvnoncat.h5', "r")
  train_set_x_orig = np.array(train_dataset["train_set_x"][:])                                            #train set features
  train_set_y_orig = np.array(train_dataset["train_set_y"][:])                                            #train set labels

  test_dataset = h5py.File('/content/drive/My Drive/Colab Notebooks/test_catvnoncat.h5', "r")
  test_set_x_orig = np.array(test_dataset["test_set_x"][:])                                               #test set features
  test_set_y_orig = np.array(test_dataset["test_set_y"][:])                                               #test set labels

  classes = np.array(test_dataset["list_classes"][:])                                                     #the list of classes
    
  train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
  test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
  m_train = train_set_x_orig.shape[0]
  m_test = test_set_x_orig.shape[0]
  num_px = train_set_x_orig.shape[1]

  train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
  test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

  train_set_x = train_set_x_flatten/255
  test_set_x = test_set_x_flatten/255

  return train_set_x, train_set_y, test_set_x, test_set_y, classes


def sigmoid(z):

  s = 1/(1+np.exp(-z))

  return s


def initialize_with_zeros(dim):
  w = np.zeros((dim, 1))
  b = 0

  assert(w.shape == (dim, 1))
  assert(isinstance(b, float) or isinstance(b, int))

  return w, b


def propagate(w, b, X, Y):

  m = X.shape[1]
  
  A = sigmoid(np.dot(w.T, X) + b)
  cost = (-1/m)* (np.dot(Y, np.log(A).T) + np.dot((1-Y), np.log(1-A).T))
  
  dw = (1/m)*(np.dot(X, (A-Y).T))
  db = (1/m)*np.sum(A-Y)

  assert(dw.shape == w.shape)
  assert(db.dtype == float)
  cost = np.squeeze(cost)
  assert(cost.shape == ())
  grads = {"dw": dw, "db": db}
    
  return grads, cost


def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

  costs = []
  
  for i in range(num_iterations):
    grads, cost = propagate(w, b, X, Y)                                         #here we use the previously created forward propogation function
    
    dw = grads["dw"]
    db = grads["db"]
        
    w = w - (learning_rate*dw)
    b = b - (learning_rate*db)

    if i % 100 == 0:
      costs.append(cost)
    
    if print_cost and i % 100 == 0:
      print ("Cost after iteration {}: {}".format(i, cost))
    
  params = {"w": w, "b": b}
  grads = {"dw": dw, "db": db}
    
  return params, grads, costs


def predict(w, b, X):
  
  m = X.shape[1]
  Y_prediction = np.zeros((1,m))
  w = w.reshape(X.shape[0], 1)

  A = sigmoid(np.dot(w.T, X) + b)

  for i in range(A.shape[1]):
    if A[0, i] > 0.5:
      Y_prediction[0, i] = 1

  assert(Y_prediction.shape == (1, m))

  return Y_prediction


def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
                                                    #here 'X_train' is the training dataset of size (num_px * num_px * 3, number of training examples m_train)
                                                    #and 'Y_train' is the true 'label' vector (containing 0 if non-cat, 1 if cat) of size (1, number of training examples m_train)
                                                    #and 'X_test' is the test dataset of size (num_px * num_px * 3, number of test examples m_test)
                                                    #and 'Y_test' is the true 'label' vector (containing 0 if non-cat, 1 if cat) of size (1, number of test examples m_test)
                                                    #and 'num_iterations' is a hyperparameter representing the number of iterations of the optimization loop with a default value of 2000
                                                    #and 'learning_rate' is a hyperparameter representing the learning rate of gradient descent update rule with a default value of 0.5
                                                    #and 'print_cost' function prints the cost function's value after every 100 steps if set to 'True'

  w, b = initialize_with_zeros(dim = X_train.shape[0])                          #here we initialize our parameters using our initialize_with_zeros() function

  parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
                                                                                #here we calculate our parameters using the gradient descent update rule using our optimize() function

  w = parameters["w"]                                                           #here we retrieve parameters 'w' and 'b' from dictionary 'parameters'
  b = parameters["b"]

  Y_prediction_test = predict(w, b, X_test)                                     #here we predict the train and test set examples using the above calculated parameters
  Y_prediction_train = predict(w, b, X_train)

  print()
  print("Train accuracy: {}%".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
  print("Test accuracy: {}%".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))
  print()

  modelinfo = {"costs": costs,                                                  #here 'modelinfo' is the dictionary containing all information about the model
               "Y_prediction_test": Y_prediction_test, 
               "Y_prediction_train" : Y_prediction_train, 
               "w" : w, 
               "b" : b,
               "learning_rate" : learning_rate,
               "num_iterations": num_iterations}
    
  return modelinfo


train_set_x, train_set_y, test_set_x, test_set_y, classes = load_dataset()

model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)
